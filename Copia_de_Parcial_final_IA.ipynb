{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jenni01-12/Parcial-Final/blob/main/Copia_de_Parcial_final_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalación de librerías**"
      ],
      "metadata": {
        "id": "GHLeTuU_3l_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Configuración Inicial ---\n",
        "# Nombre del modelo Ollama a utilizar (cambia esto para usar un modelo diferente)\n",
        "# Asegúrate de que este modelo exista en Ollama y sea adecuado para tareas de generación de código.\n",
        "# Modelos como 'codellama', 'deepseek-coder', 'llama3' suelen ser buenos para esto.\n",
        "# 'llama3' es una buena opción general.\n",
        "ollama_model_name = \"deepseek-r1\" # Prueba con llama3, o \"codellama\" si quieres enfocarte en código\n",
        "# ollama_model_name = \"llama2:7b\" # Puedes volver a llama2 si prefieres\n",
        "\n",
        "# Nombre del archivo CSV si decides usar uno real (deja None para usar datos simulados)\n",
        "# Para usar un archivo real:\n",
        "# 1. Sube tu archivo CSV a tu sesión de Colab.\n",
        "# 2. Cambia esta variable a \"nombre_de_tu_archivo.csv\"\n",
        "csv_file_name = \"Gravedad.csv\" # Cambia a \"tu_archivo.csv\" para usar un archivo real\n",
        "\n",
        "# --- 1. Instalación de Librerías y Ollama ---\n",
        "\n",
        "print(\"--- Instalando librerías Python ---\")\n",
        "# Instalamos/actualizamos pandas, numpy, ollama y gradio\n",
        "# Usamos --upgrade para asegurar versiones compatibles y -q para reducir el output\n",
        "# ¡pandas y numpy ya deberían estar bien de la vez anterior, pero --upgrade no hace daño!\n",
        "!pip install --upgrade pandas numpy ollama gradio openpyxl -q\n",
        "\n",
        "# **Verificación de importaciones**\n",
        "print(\"\\n--- Verificando importaciones básicas ---\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import ollama\n",
        "    import gradio as gr\n",
        "    import matplotlib.pyplot as plt # Para plots generados por código\n",
        "    import io\n",
        "    import sys # Importar sys para capturar stdout\n",
        "    print(\"¡pandas, numpy, ollama, gradio, matplotlib, io y sys importados correctamente!\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR CRÍTICO: No se pudo importar una librería necesaria: {e}\")\n",
        "    print(\"Asegúrate de que las instalaciones se completaron sin errores graves.\")\n",
        "    # import sys; sys.exit(\"Fallo en importaciones básicas.\") # Descomentar si quieres detenerte aquí\n",
        "except Exception as e:\n",
        "    print(f\"ERROR inesperado durante la importación: {e}\")\n",
        "    # import sys; sys.exit(\"Fallo en importaciones básicas.\") # Descomentar si quieres detenerte aquí\n",
        "\n",
        "\n",
        "# Instalar Ollama en Colab (solo es necesario ejecutar una vez por sesión)\n",
        "print(\"\\n--- Instalando Ollama ---\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# --- 2. Iniciar el servidor de Ollama en segundo plano ---\n",
        "print(\"\\n--- Iniciando servidor Ollama ---\")\n",
        "!pkill ollama || true # Asegura que no hay procesos ollama corriendo\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 & # Ejecuta en segundo plano\n",
        "\n",
        "# --- 3. Esperar a que el servidor de Ollama inicie completamente ---\n",
        "import time\n",
        "print(\"Esperando a que el servidor Ollama esté listo (15 segundos)...\")\n",
        "time.sleep(15) # Ajusta este tiempo si ves errores de conexión iniciales\n",
        "\n",
        "# --- 4. Verificar que el servidor esté respondiendo ---\n",
        "print(\"\\n--- Verificando servidor Ollama ---\")\n",
        "!curl -s http://localhost:11434/api/tags || echo \"El servidor Ollama no está respondiendo. Revisa los logs o aumenta el tiempo de espera.\"\n",
        "\n",
        "# --- 5. Descargar el modelo especificado ---\n",
        "print(f\"\\n--- Descargando modelo {ollama_model_name} desde Ollama ---\")\n",
        "# Si cambiaste ollama_model_name, esto descargará el nuevo. Si ya lo tienes, será rápido.\n",
        "!ollama pull {ollama_model_name}\n",
        "print(f\"Modelo {ollama_model_name} descargado (o ya disponible).\")\n",
        "\n",
        "# --- 6. Preparar los datos (Simulados o cargados desde CSV) ---\n",
        "# pandas y os ya deberían estar importados desde la verificación inicial\n",
        "import os\n",
        "\n",
        "print(\"\\n--- Cargando datos ---\")\n",
        "\n",
        "df = None # Inicializar el dataframe a None\n",
        "\n",
        "if csv_file_name and os.path.exists(csv_file_name):\n",
        "    # Si se especificó un archivo CSV y existe, cargarlo\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_name)\n",
        "        print(f\"Datos cargados desde '{csv_file_name}'. Filas: {len(df)}, Columnas: {len(df.columns)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el archivo CSV '{csv_file_name}': {e}\")\n",
        "        print(\"Se procederá a usar datos simulados.\")\n",
        "        csv_file_name = None # Revertir para usar datos simulados si falla la carga\n",
        "else:\n",
        "    if csv_file_name:\n",
        "        print(f\"Archivo '{csv_file_name}' no encontrado.\")\n",
        "        print(\"Se procederá a usar datos simulados.\")\n",
        "        csv_file_name = None # Revertir para usar datos simulados si no se encuentra\n"
      ],
      "metadata": {
        "id": "CfnZDyARrjst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a62228-c3a4-4900-bff0-dcde4e83d1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Instalando librerías Python ---\n",
            "\n",
            "--- Verificando importaciones básicas ---\n",
            "¡pandas, numpy, ollama, gradio, matplotlib, io y sys importados correctamente!\n",
            "\n",
            "--- Instalando Ollama ---\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "################                                                          23.3%"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrimeras 5 filas del DataFrame:\")\n",
        "print(df.head().to_markdown(index=False)) # Mostrar como markdown para mejor legibilidad en Colab\n",
        "print(f\"\\nColumnas y tipos de datos:\")\n",
        "display(df) # Usar df.info() directamente para mostrar en la consola de Colab\n",
        "# --- 7. Funciones para interactuar con Ollama y ejecutar código ---\n",
        "\n",
        "# Pre-generar la descripción del DataFrame que se enviará al LLM\n",
        "# Esto se hace una vez después de cargar o crear el DataFrame\n",
        "# Usamos io.StringIO() para capturar el output de df.info() como string para el LLM\n",
        "buffer = io.StringIO()\n",
        "df.info(verbose=True, buf=buffer)\n",
        "dataframe_description = f\"\"\"Tienes acceso a un pandas DataFrame llamado 'df'.\n",
        "Su información (columnas, tipos, no-nulos) es:\n",
        "{buffer.getvalue()}\n",
        "\n",
        "Primeras filas:\n",
        "{df.head().to_string()}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "63jXx2YItBTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_python_code(prompt, dataframe_info=dataframe_description, model=ollama_model_name):\n",
        "    \"\"\"\n",
        "    Genera código Python ejecutable sin secciones de razonamiento\n",
        "    \"\"\"\n",
        "    system_prompt = f\"\"\"Eres un generador de código Python especializado en análisis económico.\n",
        "    Reglas estrictas:\n",
        "    1. Devuelve SOLO código Python ejecutable (sin comentarios tipo <think>)\n",
        "    2. El código debe:\n",
        "       - Usar el DataFrame 'df'\n",
        "       - Crear gráficos con matplotlib/seaborn\n",
        "       - Guardar la figura en variable 'fig'\n",
        "    3. Estructura obligatoria:\n",
        "       - Importar librerías necesarias\n",
        "       - Procesamiento de datos\n",
        "       - Visualización\n",
        "       - Retorno de figura (fig = plt.gcf())\n",
        "    4. Prohibido:\n",
        "       - Secciones de razonamiento\n",
        "       - Comentarios extensos\n",
        "       - Bloques markdown\n",
        "    Datos disponibles: {dataframe_info}\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Pregunta del usuario: {prompt}\"\n",
        "\n",
        "    print(f\"\\n--- Enviando prompt a {model} para generar código ---\")\n",
        "    # Usamos la librería ollama para interactuar con el servidor local\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {'role': 'system', 'content': system_prompt},\n",
        "                {'role': 'user', 'content': user_prompt},\n",
        "            ],\n",
        "            options={\n",
        "                \"temperature\": 0.2, # Mantener bajo para código preciso\n",
        "                # \"num_predict\": 512 # Opcional: limitar longitud si es necesario\n",
        "            }\n",
        "        )\n",
        "        # Asegurarse de que solo se extrae el contenido de la respuesta del modelo\n",
        "        code = response['message']['content'].strip()\n",
        "\n",
        "        # A veces, los LLMs pueden incluir markdown a pesar de las instrucciones estrictas.\n",
        "        # Intentar limpiar bloques de código markdown si aparecen.\n",
        "        if code.startswith(\"```python\"):\n",
        "            code = code.replace(\"```python\", \"\").strip()\n",
        "            if code.endswith(\"```\"):\n",
        "                code = code[:-3].strip()\n",
        "        elif code.startswith(\"```\"): # Otros bloques de código\n",
        "             code = code.replace(\"```\", \"\").strip()\n",
        "\n",
        "\n",
        "        print(\"Código Python generado:\")\n",
        "        print(\"--- INICIO CÓDIGO ---\")\n",
        "        print(code)\n",
        "        print(\"--- FIN CÓDIGO ---\")\n",
        "        return code\n",
        "    except Exception as e:\n",
        "        print(f\"Error al llamar a Ollama para generar código: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "j_sWtrJpvdV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_python_code(code, dataframe):\n",
        "    \"\"\"\n",
        "    Ejecuta código Python con validaciones mejoradas y manejo de errores robusto.\n",
        "    Devuelve:\n",
        "    - Para gráficos: objeto matplotlib.Figure\n",
        "    - Para texto: str con output\n",
        "    - Para errores: str descriptivo\n",
        "    \"\"\"\n",
        "    # --- Validaciones iniciales ---\n",
        "    if \"<think>\" in code or \"```\" in code:\n",
        "        return \"Error: El código contiene secciones no ejecutables (remove <think> o markdown)\"\n",
        "\n",
        "    required_vars = {'pd', 'plt', 'df'}\n",
        "    if not all(var in code for var in required_vars):\n",
        "        return f\"Error: Faltan imports esenciales. Requeridos: {required_vars}\"\n",
        "    print(\"\\n--- Ejecutando código Python ---\")\n",
        "    # Usamos exec para ejecutar código en un entorno controlado\n",
        "    # Creamos un diccionario para el entorno de ejecución, incluyendo el DataFrame 'df' y librerías comunes\n",
        "    execution_env = {'df': dataframe, 'pd': pd, 'np': np, 'plt': plt} # Usamos 'np' y 'plt'\n",
        "\n",
        "    # Redirigir stdout para capturar output de print\n",
        "    old_stdout = sys.stdout\n",
        "    redirected_output = io.StringIO()\n",
        "    sys.stdout = redirected_output\n",
        "\n",
        "    # Usamos un diccionario para capturar el resultado si el código lo asigna a una variable 'result'\n",
        "    # o si la última expresión no es print.\n",
        "    result_holder = {'_result': None}\n",
        "    code_to_exec = f\"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n{code}\\n\\n_result = None\\ntry:\\n    _result = eval(compile(code_to_exec, '<string>', 'eval'), execution_env)\\nexcept:\\n    pass # Could not evaluate as single expression\"\n",
        "     # ^^ Intentamos envolverlo para capturar la última expresión, pero esto puede ser frágil.\n",
        "     # Un enfoque más robusto es confiar en que el LLM use print() o retorne una figura.\n",
        "\n",
        "    # Simplificamos la ejecución para confiar más en el `print` del código generado o en el retorno de la figura\n",
        "    try:\n",
        "        # Ejecutar el código en el entorno preparado\n",
        "        exec(code, execution_env)\n",
        "\n",
        "        # Restaurar stdout\n",
        "        sys.stdout = old_stdout\n",
        "        output_text = redirected_output.getvalue().strip() # Obtener output impreso\n",
        "\n",
        "        # Intentar detectar si se generó un plot\n",
        "        fig = None\n",
        "        try:\n",
        "            # Si hay figuras activas y no se cerraron explícitamente\n",
        "            # get_fignums() devuelve una lista de ids de figuras activas\n",
        "            if plt.get_fignums():\n",
        "                 # Asumimos que la última figura creada es la relevante\n",
        "                 fig = plt.gcf() # Get Current Figure\n",
        "                 print(\"(Código ejecutado, se detectó una figura de matplotlib)\") # Mensaje para el log\n",
        "                 # Es buena práctica limpiar las figuras después de obtener la que queremos\n",
        "                 # plt.close('all') # Podríamos cerrarlas aquí, o después de retornarlas en el handler de Gradio\n",
        "        except Exception as fig_e:\n",
        "             print(f\"Advertencia al verificar figuras de matplotlib: {fig_e}\")\n",
        "             pass # No fallar si falla la detección de figura\n",
        "\n",
        "        # Determinar el resultado a devolver\n",
        "        if fig:\n",
        "            # Si hay una figura, devolver la figura (Gradio la mostrará en el componente Plot)\n",
        "            return fig\n",
        "        elif output_text:\n",
        "            # Si hay output de texto, devolverlo (Gradio lo mostrará en el componente Textbox)\n",
        "            return output_text\n",
        "        else:\n",
        "            # Si no hay ni figura ni output de texto\n",
        "            # Podríamos intentar devolver el valor de result_holder['_result'] si lo implementamos,\n",
        "            # pero basándonos en las instrucciones del prompt, el LLM debería imprimir o generar figura.\n",
        "            # Retornar un mensaje informativo.\n",
        "            return \"Código ejecutado. No se produjo output de texto ni figura visible. Revisa el prompt o el código generado.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Restaurar stdout si ocurrió un error antes de restaurarlo\n",
        "        sys.stdout = old_stdout\n",
        "        # Limpiar figuras si hubo un error para evitar que interfieran en la siguiente ejecución\n",
        "        plt.close('all')\n",
        "        print(f\"Error al ejecutar el código: {e}\")\n",
        "        # Devolver el mensaje de error a la interfaz\n",
        "        # Incluimos el código ejecutado para depuración en la interfaz\n",
        "        return f\"ERROR al ejecutar el código:\\n{e}\\n\\nCódigo ejecutado:\\n```python\\n{code}\\n```\""
      ],
      "metadata": {
        "id": "PeoocYuCxLCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_generated_plot(figure, prompt_context, model=ollama_model_name):\n",
        "    \"\"\"Analiza un gráfico matplotlib ya generado y devuelve insights\"\"\"\n",
        "    import tempfile\n",
        "    import base64\n",
        "    import os\n",
        "\n",
        "    # Guardar el gráfico temporalmente\n",
        "    temp_img = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
        "    figure.savefig(temp_img.name, bbox_inches='tight')\n",
        "    temp_img.close()\n",
        "\n",
        "    # Leer la imagen en base64\n",
        "    with open(temp_img.name, 'rb') as img_file:\n",
        "        encoded_img = base64.b64encode(img_file.read()).decode('utf-8')\n",
        "\n",
        "    system_prompt = \"\"\"Eres un intérprete visual de datos económicos. Analiza el gráfico proporcionado y:\n",
        "    1. Identifica patrones clave (máx. 3)\n",
        "    2. Explica su relevancia económica\n",
        "    3. Destaca anomalías (si existen)\n",
        "\n",
        "    Formato de respuesta:\n",
        "    - Patrón 1: [descripción] → [significado económico]\n",
        "    - Patrón 2: [descripción] → [significado económico]\n",
        "    - Anomalía: [descripción] → [posible causa]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {'role': 'system', 'content': system_prompt},\n",
        "                {'role': 'user', 'content': [\n",
        "                    {'type': 'text', 'text': f\"Contexto: {prompt_context}\"},\n",
        "                    {'type': 'image_url', 'image_url': f\"data:image/png;base64,{encoded_img}\"}\n",
        "                ]}\n",
        "            ]\n",
        "        )\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error analizando gráfico: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        os.unlink(temp_img.name)\n",
        "\n",
        "def generate_markdown_summary(plot_analysis, prompt_context):\n",
        "    \"\"\"Genera resumen para PPT en markdown basado en el análisis\"\"\"\n",
        "    system_prompt = \"\"\"Genera un resumen ejecutivo en markdown para PowerPoint con:\n",
        "    # Título\n",
        "    - **Hallazgos**: 3 bullet points\n",
        "    - **Conclusión**: 1 frase\n",
        "    - **Recomendación**: 1 acción\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model=ollama_model_name,\n",
        "            messages=[\n",
        "                {'role': 'system', 'content': system_prompt},\n",
        "                {'role': 'user', 'content': f\"Análisis: {plot_analysis}\\nContexto: {prompt_context}\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando resumen: {e}\")\n",
        "        return \"## Error\\nNo se pudo generar el resumen\""
      ],
      "metadata": {
        "id": "RcdJNDsPblel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función que Gradio llamará\n",
        "def chatbot_response(user_input):\n",
        "    \"\"\"Procesa la entrada del usuario, genera y ejecuta código, y devuelve: texto, gráfico y resumen markdown\"\"\"\n",
        "    global df # Asegúrate de que df está accesible (definido en la Celda 6)\n",
        "\n",
        "    if df is None:\n",
        "        return \"Error: DataFrame no cargado. Verifica la configuración.\", None, None # Retornar 3 valores (texto, None)\n",
        "\n",
        "    # Opciones especiales para la interfaz (opcional, puedes quitar si no las necesitas)\n",
        "    if user_input.lower() == 'mostrar dataframe info':\n",
        "        buffer = io.StringIO()\n",
        "        df.info(verbose=True, buf=buffer)\n",
        "        return buffer.getvalue(), None, None\n",
        "    if user_input.lower() == 'mostrar primeras filas':\n",
        "        return df.head().to_markdown(index=False), None, None\n",
        "    if user_input.lower() == 'mostrar datos simulados':\n",
        "        return df.to_markdown(index=False), None, None\n",
        "\n",
        "\n",
        "    # Generar el código Python usando Ollama\n",
        "    code_to_execute = generate_python_code(user_input, dataframe_description, model=ollama_model_name)\n",
        "\n",
        "    # Limpiar figuras existentes antes de ejecutar un nuevo código que podría crear plots\n",
        "    plt.close('all')\n",
        "\n",
        "    if code_to_execute:\n",
        "        # Ejecutar el código generado\n",
        "        execution_result = execute_python_code(code_to_execute, df)\n",
        "\n",
        "    #creamos las funciones\n",
        "    markdown_summary = None\n",
        "    analysis_text = None\n",
        "\n",
        "        # Gradio puede manejar la distinción entre texto y figura si la función retorna el tipo correcto\n",
        "        # Si execution_result es un string, Gradio lo pondrá en el primer output (Textbox)\n",
        "        # Si execution_result es un Figure, Gradio lo pondrá en el segundo output (Plot)\n",
        "        # Debemos devolver una tupla (texto, plot) donde uno es el resultado y el otro es None\n",
        "    if isinstance(execution_result, plt.Figure):\n",
        "        # 1. Análisis del gráfico (nuevo)\n",
        "        plot_analysis = analyze_generated_plot(execution_result, user_input)\n",
        "\n",
        "        if plot_analysis:\n",
        "            analysis_text = f\"**Análisis del gráfico**:\\n{plot_analysis}\"\n",
        "            # 2. Generar resumen markdown (nuevo)\n",
        "            markdown_summary = generate_markdown_summary(plot_analysis, user_input)\n",
        "\n",
        "        return analysis_text, execution_result, markdown_summary\n",
        "\n",
        "    elif isinstance(execution_result, str):\n",
        "        return execution_result, None, None  # Mantiene output original + markdown None\n",
        "\n",
        "    else:\n",
        "        return f\"Resultado inesperado: {type(execution_result)}\", None, None\n",
        "\n",
        "\n",
        "print(\"\\n--- Configurando interfaz Gradio ---\")\n",
        "\n",
        "# Crear la interfaz Gradio\n",
        "if df is not None: # Solo si el DataFrame se cargó correctamente\n",
        "    interface = gr.Interface(\n",
        "        fn=chatbot_response, # La función a llamar\n",
        "        inputs=gr.Textbox(label=\"Tu pregunta sobre los datos o comando especial\"), # Un campo de texto para la entrada\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Análisis\", interactive=False, visible=True), # Campo para respuestas de texto, siempre visible\n",
        "            gr.Plot(label=\"Gráfico\", visible=True), # Campo para gráficos, siempre visible\n",
        "            gr.Markdown(label=\"Resumen PPT\", visible=True)  # Aseguramos que sea visible\n",
        "            # Gradio ocultará automáticamente los componentes si el valor retornado es None\n",
        "        ],\n",
        "        title=\"CSV Chatbot Mejorado\",\n",
        "        description=f\"\"\"Ahora con análisis automático y resumen para presentaciones.<br>\n",
        "        Columnas: {', '.join(df.columns)}. Modelo: {ollama_model_name}<br>\n",
        "        Comandos especiales: `mostrar dataframe info`, `mostrar primeras filas`\"\"\",\n",
        "        live=False,\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Iniciando interfaz Gradio ---\")\n",
        "    # share=True genera un enlace público de Hugging Face temporal.\n",
        "    # debug=True muestra logs de Gradio (útil para depurar).\n",
        "    # El puerto 7860 es el predeterminado de Gradio, asegúrate de que no esté en uso.\n",
        "    # Usamos `inline=False` para forzar que el enlace se muestre claramente, no incrustado.\n",
        "    interface.launch(share=True, debug=True, inline=False)\n",
        "\n",
        "    print(\"\\n--- Interfaz Gradio iniciada. Busca el enlace 'Public URL' en el output de la celda. ---\")\n",
        "else:\n",
        "    print(\"\\nDataFrame no cargado. No se puede iniciar la interfaz Gradio.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GBaUmSlhdGoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Configuración inicial\n",
        "!pip install -q gradio ollama pandas matplotlib\n",
        "\n",
        "# 2. Inicia Ollama (si usas modelo local)\n",
        "!ollama serve &  # El '&' lo ejecuta en segundo plano\n",
        "\n",
        "# 3. Lanza la interfaz\n",
        "interface = gr.Interface(your_function, inputs, outputs)\n",
        "interface.launch(share=True)  # share=True es clave para enlace público"
      ],
      "metadata": {
        "id": "HXXWzDr8sDuL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}